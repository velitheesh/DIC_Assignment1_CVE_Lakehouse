# ğŸŒ CVE Lakehouse Pipeline using Databricks Community Edition

A complete end-to-end Lakehouse data engineering project that ingests, processes, and analyzes 2024 CVE (Common Vulnerabilities & Exposures) records using Databricks Community Edition, Delta Lake, and SQL-based analytics.

This repository demonstrates the Bronze â†’ Silver â†’ Gold Lakehouse Architecture, optimized for Databricks CE filesystem limitations.

---

## ğŸ—ï¸ 1. Architecture Overview

### ğŸŸ« Bronze Layer â€” Raw Ingestion

| Step | Description |
| :--- | :--- |
| **ğŸ“¥ Download** | Extracts the `cvelistV5` GitHub repository |
| **ğŸ“„ Load JSON** | Loads all CVE JSON files using `recursiveFileLookup` |
| **ğŸ” Filter** | Keeps only CVEs published in 2024 |
| **âœ” Quality Checks** | â‰¥ 30,000 records, no null CVE IDs, IDs unique |
| **ğŸ’¾ Store** | Bronze Delta table created in `/databricks/driver` |

### ğŸ¥ˆ Silver Layer â€” Normalization

**1ï¸âƒ£ core Table**
Contains normalized CVE metadata:
* CVE ID
* Date published
* Date reserved
* Description
* CVSS score
* CVSS severity
* Status

**2ï¸âƒ£ affected Table**
Explodes nested JSON structures:

| Column | Meaning |
| :--- | :--- |
| `cve_id` | Vulnerability identifier |
| `vendor` | Vendor or organization impacted |
| `product` | Product, software, or component |

*Enables high-quality vendor-level analytics.*

### ğŸ¥‡ Gold Layer â€” SQL EDA

Analysis performed on the Silver tables:
* ğŸ“Š Top affected vendors
* ğŸ§­ CVSS severity distribution
* â³ Disclosure lag calculations
* ğŸ“ˆ Summary statistics (mean, stddev, max, etc.)

---

## ğŸ—‚ï¸ 2. Repository Structure

```text
CVE_Lakehouse_Assignment/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ Report/
â”‚   â””â”€â”€ CVE_Lakehouse_Final_Report.pdf
â”‚
â”œâ”€â”€ Notebooks/
â”‚   â”œâ”€â”€ 01_bronze_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_silver_transformations.ipynb
â”‚   â””â”€â”€ 03_sql_eda.ipynb
â”‚
â”œâ”€â”€ Screenshots/
â”‚   â”œâ”€â”€ bronze_01_count.jpeg
â”‚   â”œâ”€â”€ bronze_02_quality_checks.jpeg
â”‚   â”œâ”€â”€ bronze_03_describe_detail.jpeg
â”‚   â”œâ”€â”€ silver_01_describe_core.jpeg
â”‚   â”œâ”€â”€ silver_02_describe_affected.jpeg
â”‚   â”œâ”€â”€ gold_01_temporal_lag.jpeg
â”‚   â”œâ”€â”€ gold_02_risk_distribution.jpeg
â”‚   â””â”€â”€ gold_03_top_vendors.jpeg
â”‚
â””â”€â”€ SQL_Queries/
    â””â”€â”€ eda_queries.sql
```
---

## âš™ï¸ 3. Environment Details

| Component | Details |
|----------|---------|
| **Platform** | Databricks Community Edition |
| **Language** | Python (PySpark), SQL |
| **Storage** | `/databricks/driver` (required for CE) |
| **Runtime** | Latest CE-supported Databricks Runtime |
| **Delta Lake** | Yes â€” used for Bronze/Silver tables |

### âš ï¸ Important Notes  
- **Databricks CE blocks DBFS root (`dbfs:/`) access**.  
- All ingestion **must** use the **driver filesystem**  
  (`/databricks/driver/...`).  

---

## ğŸš€ 4. How to Run the Pipeline

### ğŸ”¹ STEP 1 â€” Setup  
1. Create a Databricks CE cluster  
2. Import all three notebooks:  
   - `Bronze Notebook`  
   - `Silver Notebook`  
   - `Gold (SQL EDA) Notebook`  
3. Attach all notebooks to the cluster

---

### ğŸ”¹ STEP 2 â€” Run **Bronze Notebook**

This notebook performs:

- **Download ZIP** â€” Retrieve *cvelistV5* repository  
- **Extract** â€” Unzip into `/databricks/driver`  
- **Load JSON** â€” Read all nested CVE JSON files  
- **Filter** â€” Keep only **2024 CVEs**  
- **Validate** â€” Schema + data quality checks  
- **Save** â€” Store as **Bronze Delta table**

---

### ğŸ”¹ STEP 3 â€” Run **Silver Notebook**

Creates two refined Delta tables:

| Table | Description |
|-------|-------------|
| `cve_silver.core` | Normalized core CVE attributes |
| `cve_silver.affected` | Vendor, product, version breakdown |

---

### ğŸ”¹ STEP 4 â€” Run **Gold (SQL EDA) Notebook**

Produces analytics:

- Severity distribution  
- Vendor-based risk rankings  
- Disclosure lag statistics  
- Summary statistics tables  

---

## ğŸ“Š 5. Key Insights (Summary)

### ğŸ† Top Affected Vendors  
| Vendor | Count |
|--------|-------|
| **Microsoft** | 13,161 |
| **n/a** | 6,591 |
| **Linux** | 6,152 |

---

### ğŸ”¥ Severity Distribution  

| Severity | Count | % |
|---------|-------|---------|
| **Medium** | 11,795 | ~30% |
| **High** | 7,588 | ~19% |
| **Critical** | 1,788 | ~4.6% |
| **Low** | 1,015 | ~2.6% |

---

### â³ Disclosure Lag Analysis  
- **Average lag:** 50.83 days  
- **Maximum lag:** 686 days  

**Observation:**  
Disclosure timelines vary significantly â€” suggesting inconsistent security reporting practices among vendors.

---

## ğŸ“ 6. Conclusion

This project demonstrates a fully functional **Lakehouse data engineering pipeline** built entirely on **Databricks Community Edition**, overcoming CEâ€™s DBFS limitations.

### âœ”ï¸ Key Accomplishments

- Ingested & normalized **thousands of nested CVE JSON files**  
- Implemented **Bronze â†’ Silver â†’ Gold** Delta Lake patterns  
- Executed SQL EDA to derive meaningful cybersecurity insights  
- Designed a **reproducible, scalable pipeline architecture**  
- Followed **modern industry standards** for:
  - Data engineering  
  - Delta Lake modeling  
  - Security analytics  

---


