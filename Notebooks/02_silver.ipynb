{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1c6d97a-c679-4f46-ab2e-ffba14502de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 1: Initialize Silver Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "743925f7-219a-40bb-8b8e-f888514419e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Bronze Delta files directly from: /Volumes/workspace/default/cve_demo/bronze/2024_Nov13\n✅ Successfully loaded Delta files: 38,753 records.\n✅ Bronze table re-registered successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_json, coalesce, to_timestamp, explode_outer\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# --- IMPORTANT: PASTE YOUR ACTUAL DELTA FILE PATH HERE ---\n",
    "# This path was likely printed in the output of your original Bronze notebook (Cell 5, Output 3/3).\n",
    "# Example format: \"/Volumes/workspace/default/cve_demo/bronze/2024_Nov13\"\n",
    "# You MUST replace the placeholder below with your actual path.\n",
    "ACTUAL_DELTA_FILE_PATH = \"/Volumes/workspace/default/cve_demo/bronze/2024_Nov13\"\n",
    "# --------------------------------------------------------\n",
    "\n",
    "print(f\"1. Loading Bronze Delta files directly from: {ACTUAL_DELTA_FILE_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Read the data directly from the Delta files on disk.\n",
    "    bronze_df = spark.read.format(\"delta\").load(ACTUAL_DELTA_FILE_PATH)\n",
    "    print(f\"✅ Successfully loaded Delta files: {bronze_df.count():,} records.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ ERROR: Could not read Delta files. Please verify the file path.\")\n",
    "    raise e\n",
    "\n",
    "# --- Re-register Bronze Table for convenience and correctness ---\n",
    "# This re-establishes the 'cve_bronze.records' table for the current session.\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS cve_bronze\")\n",
    "\n",
    "# Overwrite (re-register) the in-memory DataFrame as the official table\n",
    "(bronze_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"cve_bronze.records\")\n",
    ")\n",
    "print(\"✅ Bronze table re-registered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "111fb332-7738-40f2-81ca-8bcb14d1d912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Create the cve_silver.core Table (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef347e5-82b8-48f4-9e09-1ba83c2b9608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Delta files directly from: /Volumes/workspace/default/cve_demo/bronze/2024_Nov13\n✅ Table 'cve_silver.core' registered.\n✅ Table 'cve_silver.affected' registered.\n\n--- Silver Layer Verification ---\n\uD83D\uDCCA Silver Core Records: 38,753\n\uD83D\uDCCA Silver Affected Records: 73,998\n\n--- DESCRIBE DETAIL: cve_silver.core ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>clusteringColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th><th>tableFeatures</th><th>statistics</th><th>clusterByAuto</th></tr></thead><tbody><tr><td>delta</td><td>3abe94f6-eb0a-474e-9f48-a4f210149a36</td><td>workspace.cve_silver.core</td><td>null</td><td></td><td>2025-11-17T16:00:42.367Z</td><td>2025-11-17T16:00:46.000Z</td><td>List()</td><td>List()</td><td>1</td><td>4451977</td><td>Map(delta.parquet.compression.codec -> zstd, delta.enableDeletionVectors -> true)</td><td>3</td><td>7</td><td>List(appendOnly, deletionVectors, invariants)</td><td>Map(numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0)</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "delta",
         "3abe94f6-eb0a-474e-9f48-a4f210149a36",
         "workspace.cve_silver.core",
         null,
         "",
         "2025-11-17T16:00:42.367Z",
         "2025-11-17T16:00:46.000Z",
         [],
         [],
         1,
         4451977,
         {
          "delta.enableDeletionVectors": "true",
          "delta.parquet.compression.codec": "zstd"
         },
         3,
         7,
         [
          "appendOnly",
          "deletionVectors",
          "invariants"
         ],
         {
          "numDeletionVectors": 0,
          "numRowsDeletedByDeletionVectors": 0
         },
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "format",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "createdAt",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "lastModified",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "partitionColumns",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        },
        {
         "metadata": "{}",
         "name": "clusteringColumns",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        },
        {
         "metadata": "{}",
         "name": "numFiles",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sizeInBytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "properties",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "minReaderVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "minWriterVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "tableFeatures",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        },
        {
         "metadata": "{}",
         "name": "statistics",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"long\"}"
        },
        {
         "metadata": "{}",
         "name": "clusterByAuto",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_json, explode_outer, coalesce, to_timestamp\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# --- IMPORTANT: PASTE YOUR ACTUAL DELTA FILE PATH HERE ---\n",
    "# This path was printed in the output of your original Bronze notebook.\n",
    "ACTUAL_DELTA_FILE_PATH = \"/Volumes/workspace/default/cve_demo/bronze/2024_Nov13\"\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# --- Schemas for JSON Parsing (The FIX for both errors) ---\n",
    "\n",
    "# 1. Schema for cveMetadata column\n",
    "cve_metadata_schema = StructType([\n",
    "    StructField(\"cveId\", StringType(), True),\n",
    "    StructField(\"datePublished\", StringType(), True),\n",
    "    StructField(\"dateReserved\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 2. Schema for the top-level containers object (We parse the outer layer to access 'cna')\n",
    "containers_schema = StructType([\n",
    "    StructField(\"cna\", StructType([\n",
    "        StructField(\"descriptions\", ArrayType(StructType([StructField(\"value\", StringType(), True)])), True),\n",
    "        StructField(\"metrics\", ArrayType(StringType()), True),\n",
    "        StructField(\"affected\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# 3. Schema for nested objects inside containers\n",
    "affected_array_schema = ArrayType(StructType([\n",
    "    StructField(\"vendor\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"versions\", ArrayType(StructType([\n",
    "        StructField(\"version\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True)\n",
    "    ])), True)\n",
    "]))\n",
    "metrics_schema = \"struct<cvssV3_1:struct<baseScore:double, baseSeverity:string>, cvssV3_0:struct<baseScore:double, baseSeverity:string>>\"\n",
    "\n",
    "\n",
    "# --- 1. Load Data, Re-register Bronze, and Parse JSON Columns ---\n",
    "\n",
    "print(f\"1. Loading Delta files directly from: {ACTUAL_DELTA_FILE_PATH}\")\n",
    "bronze_df = spark.read.format(\"delta\").load(ACTUAL_DELTA_FILE_PATH)\n",
    "\n",
    "# Re-register Bronze table\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS cve_bronze\")\n",
    "(bronze_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"cve_bronze.records\")\n",
    ")\n",
    "\n",
    "# Apply the JSON parsing fix to BOTH cveMetadata and containers\n",
    "parsed_df = spark.table(\"cve_bronze.records\") \\\n",
    "    .withColumn(\"cveMetadata_parsed\", from_json(col(\"cveMetadata\"), cve_metadata_schema)) \\\n",
    "    .withColumn(\"containers_parsed\", from_json(col(\"containers\"), containers_schema)) \\\n",
    "    .drop(\"cveMetadata\", \"containers\") # Drop old string columns\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS cve_silver\")\n",
    "\n",
    "\n",
    "# --- 2. Create the cve_silver.core Table (Normalization) ---\n",
    "\n",
    "core_df = parsed_df.select(\n",
    "    # Use the parsed columns (FIXED)\n",
    "    col(\"cveMetadata_parsed.cveId\").alias(\"cve_id\"),\n",
    "    to_timestamp(col(\"cveMetadata_parsed.datePublished\")).alias(\"date_published\"),\n",
    "    to_timestamp(col(\"cveMetadata_parsed.dateReserved\")).alias(\"date_reserved\"),\n",
    "    col(\"cveMetadata_parsed.state\").alias(\"status\"),\n",
    "    \n",
    "    # Extract Description\n",
    "    col(\"containers_parsed.cna.descriptions\")[0][\"value\"].alias(\"description\"),\n",
    "    \n",
    "    # Parse metrics (using the nested JSON metrics_schema)\n",
    "    from_json(col(\"containers_parsed.cna.metrics\")[0], metrics_schema).alias(\"parsed_metrics\")\n",
    ").select(\n",
    "    \"*\",\n",
    "    coalesce(col(\"parsed_metrics.cvssV3_1.baseScore\"), col(\"parsed_metrics.cvssV3_0.baseScore\")).alias(\"cvss_score\"),\n",
    "    coalesce(col(\"parsed_metrics.cvssV3_1.baseSeverity\"), col(\"parsed_metrics.cvssV3_0.baseSeverity\")).alias(\"cvss_severity\")\n",
    ").drop(\"parsed_metrics\")\n",
    "\n",
    "# Register core table\n",
    "(core_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"cve_silver.core\")\n",
    ")\n",
    "print(\"✅ Table 'cve_silver.core' registered.\")\n",
    "\n",
    "\n",
    "# --- 3. Create the cve_silver.affected Table (Explode Operation) ---\n",
    "\n",
    "affected_df = parsed_df.select(\n",
    "    col(\"cveMetadata_parsed.cveId\").alias(\"cve_id\"),\n",
    "    # Parse the affected string (inside the parsed containers struct) then explode\n",
    "    from_json(col(\"containers_parsed.cna.affected\"), affected_array_schema).alias(\"affected_list\")\n",
    ").select(\n",
    "    col(\"cve_id\"),\n",
    "    explode_outer(col(\"affected_list\")).alias(\"affected_item\")\n",
    ").select(\n",
    "    col(\"cve_id\"),\n",
    "    col(\"affected_item.vendor\").alias(\"vendor\"),\n",
    "    col(\"affected_item.product\").alias(\"product\")\n",
    ").filter(col(\"vendor\").isNotNull()) \n",
    "\n",
    "# Register affected table\n",
    "(affected_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"cve_silver.affected\")\n",
    ")\n",
    "print(\"✅ Table 'cve_silver.affected' registered.\")\n",
    "\n",
    "\n",
    "# --- 4. Final Verification (Required for Deliverables) ---\n",
    "\n",
    "core_count = spark.table(\"cve_silver.core\").count()\n",
    "affected_count = spark.table(\"cve_silver.affected\").count()\n",
    "\n",
    "print(f\"\\n--- Silver Layer Verification ---\")\n",
    "print(f\"\uD83D\uDCCA Silver Core Records: {core_count:,}\")\n",
    "print(f\"\uD83D\uDCCA Silver Affected Records: {affected_count:,}\")\n",
    "\n",
    "print(\"\\n--- DESCRIBE DETAIL: cve_silver.core ---\")\n",
    "spark.sql(\"DESCRIBE DETAIL cve_silver.core\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f60ba62-e95f-43b7-b9d9-55632a0f6d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}